#cloud-config
runcmd:
  - apt-get install linux-headers-$(uname -r)
  - distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\.//g')
  - wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-keyring_1.0-1_all.deb
  - dpkg -i cuda-keyring_1.0-1_all.deb
  - apt-get update
  - apt-get -y install cuda-drivers
  - apt install -y python3-pip
  - pip3 install transformers accelerate
  - python3 -c 'import torch; from transformers import AutoModelForCausalLM, AutoTokenizer; AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-1b", device_map="auto", torch_dtype=torch.float16); AutoTokenizer.from_pretrained("cyberagent/open-calm-1b")'
  - python3 -c 'import torch; from transformers import AutoModelForCausalLM, AutoTokenizer; AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-3b", device_map="auto", torch_dtype=torch.float16); AutoTokenizer.from_pretrained("cyberagent/open-calm-3b")'
  - python3 -c 'import torch; from transformers import AutoModelForCausalLM, AutoTokenizer; AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-7b", device_map="auto", torch_dtype=torch.float16); AutoTokenizer.from_pretrained("cyberagent/open-calm-7b")'
  - cd /home/ubuntu
  - git clone https://github.com/tar-xzvff/OpenCALM-chat-api.git
  - cd OpenCALM-chat-api
  - pip install -r requirements.txt
  - python3 -c 'import torch; from transformers import AutoModelForCausalLM, AutoTokenizer; model = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-7b", device_map="auto", torch_dtype=torch.float16); model.save_pretrained("/home/ubuntu/model/cyberagent/open-calm-7"); tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-7b"); tokenizer.save_pretrained("/home/ubuntu/model/cyberagent/open-calm-7")'
  - cp systemd/opencalm_api.service /etc/systemd/system/opencalm_api.service
  - systemctl enable opencalm_api.service
  - systemctl start opencalm_api.service
